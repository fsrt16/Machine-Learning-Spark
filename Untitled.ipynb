{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+-----------------+---+----+----+--------+----+\n",
      "|Serial No.|GRE Score|TOEFL Score|University Rating|SOP|LOR |CGPA|Research|  cA|\n",
      "+----------+---------+-----------+-----------------+---+----+----+--------+----+\n",
      "|         1|      337|        118|                4|4.5| 4.5|9.65|       1|0.92|\n",
      "|         2|      324|        107|                4|4.0| 4.5|8.87|       1|0.76|\n",
      "|         3|      316|        104|                3|3.0| 3.5| 8.0|       1|0.72|\n",
      "|         4|      322|        110|                3|3.5| 2.5|8.67|       1| 0.8|\n",
      "|         5|      314|        103|                2|2.0| 3.0|8.21|       0|0.65|\n",
      "|         6|      330|        115|                5|4.5| 3.0|9.34|       1| 0.9|\n",
      "|         7|      321|        109|                3|3.0| 4.0| 8.2|       1|0.75|\n",
      "|         8|      308|        101|                2|3.0| 4.0| 7.9|       0|0.68|\n",
      "|         9|      302|        102|                1|2.0| 1.5| 8.0|       0| 0.5|\n",
      "|        10|      323|        108|                3|3.5| 3.0| 8.6|       0|0.45|\n",
      "+----------+---------+-----------+-----------------+---+----+----+--------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dataset = pd.read_csv('Admission_Predict.csv')\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "#dataset_ps = spark.createDataFrame(dataset)\n",
    "df=spark.read.csv('Admission_Predict.csv',inferSchema=True,header=True)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>cA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.6</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research   cA\n",
       "0        337          118                  4  4.5   4.6  9.65         1  0.8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = pd.DataFrame([[337,118,4,4.5,4.6,9.65,1,0.8]] , columns =['GRE Score','TOEFL Score','University Rating','SOP','LOR ','CGPA','Research','cA'])\n",
    "sp = spark.createDataFrame(d1)\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+-----------------+---+----+----+--------+----+\n",
      "|Serial No.|GRE Score|TOEFL Score|University Rating|SOP|LOR |CGPA|Research|cA  |\n",
      "+----------+---------+-----------+-----------------+---+----+----+--------+----+\n",
      "|1         |337      |118        |4                |4.5|4.5 |9.65|1       |0.92|\n",
      "|2         |324      |107        |4                |4.0|4.5 |8.87|1       |0.76|\n",
      "|3         |316      |104        |3                |3.0|3.5 |8.0 |1       |0.72|\n",
      "|4         |322      |110        |3                |3.5|2.5 |8.67|1       |0.8 |\n",
      "|5         |314      |103        |2                |2.0|3.0 |8.21|0       |0.65|\n",
      "+----------+---------+-----------+-----------------+---+----+----+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Vectorizing Streams of Data Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import VectorAssembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Serial No.: int, GRE Score: int, TOEFL Score: int, University Rating: int, SOP: double, LOR : double, CGPA: double, Research: int, cA: double, features: vector]\n",
      "DataFrame[GRE Score: bigint, TOEFL Score: bigint, University Rating: bigint, SOP: double, LOR : double, CGPA: double, Research: bigint, cA: double, features: vector]\n"
     ]
    }
   ],
   "source": [
    "vc = VectorAssembler(inputCols = ['GRE Score','TOEFL Score','University Rating','SOP','LOR ','CGPA','Research'] \n",
    "                     , outputCol = 'features')\n",
    "features_df = vc.transform(df)\n",
    "print(features_df)\n",
    "sp1=vc.transform(sp)\n",
    "print(sp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Selecting Features and the prediction columns \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[features: vector, cA: double]\n",
      "DataFrame[features: vector, cA: double]\n"
     ]
    }
   ],
   "source": [
    "model_df  = features_df.select('features','cA') \n",
    "sp1_mod = features_df.select('features','cA')\n",
    "print(sp1_mod)\n",
    "print(model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:--\n",
      "DataFrame[features: vector, cA: double]\n",
      "train:--\n",
      "DataFrame[features: vector, cA: double]\n",
      "model:--\n",
      "DataFrame[features: vector, cA: double]\n"
     ]
    }
   ],
   "source": [
    "train_df , test_df = model_df.randomSplit([0.7,0.3])\n",
    "print(\"test:--\")\n",
    "print(test_df)\n",
    "print(\"train:--\")\n",
    "print(train_df)\n",
    "print(\"model:--\")\n",
    "print(model_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|prediction        |\n",
      "+------------------+\n",
      "|0.9340921821774056|\n",
      "+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = RandomForestRegressor(labelCol = 'cA' , featuresCol ='features')\n",
    "dt_model = dt.fit(model_df)\n",
    "dt_predictions = dt_model.transform(sp1_mod)\n",
    "\n",
    "dt_predictions.select(['prediction']).show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Linear Regression \n",
    "\n",
    "\n",
    "Output follows Gaussian Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|prediction       |\n",
      "+-----------------+\n",
      "|0.955110737114637|\n",
      "+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(labelCol = 'cA' )\n",
    "lr_model = lr.fit(train_df)\n",
    "lr_predictions = lr_model.transform(sp1_mod)\n",
    "lr_predictions.select(['prediction']).show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----+-------------------+\n",
      "|features                          |cA  |prediction         |\n",
      "+----------------------------------+----+-------------------+\n",
      "|[294.0,93.0,1.0,1.5,2.0,7.36,0.0] |0.46|0.45185440159084966|\n",
      "|[295.0,96.0,2.0,1.5,2.0,7.34,0.0] |0.47|0.46441277021823146|\n",
      "|[296.0,95.0,2.0,3.0,2.0,7.54,1.0] |0.44|0.5014180043943184 |\n",
      "|[296.0,99.0,2.0,3.0,3.5,7.28,0.0] |0.47|0.4897996985108175 |\n",
      "|[298.0,92.0,1.0,2.0,2.0,7.88,0.0] |0.51|0.5173059907846305 |\n",
      "|[298.0,98.0,2.0,4.0,3.0,8.03,0.0] |0.34|0.5688787713670138 |\n",
      "|[298.0,101.0,2.0,1.5,2.0,7.86,0.0]|0.54|0.5525889952399594 |\n",
      "|[299.0,96.0,2.0,1.5,2.0,7.86,0.0] |0.54|0.5364313606818898 |\n",
      "|[300.0,99.0,1.0,1.0,2.5,8.01,0.0] |0.58|0.577413586849211  |\n",
      "|[301.0,98.0,1.0,2.0,3.0,8.03,1.0] |0.67|0.6045552546456838 |\n",
      "+----------------------------------+----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = lr_model.evaluate(train_df).predictions\n",
    "test = lr_model.evaluate(test_df).predictions\n",
    "spcase = lr_model.evaluate(sp1_mod).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----+------------------+\n",
      "|features                          |cA  |prediction        |\n",
      "+----------------------------------+----+------------------+\n",
      "|[290.0,100.0,1.0,1.5,2.0,7.56,0.0]|0.47|0.496688774986483 |\n",
      "|[290.0,104.0,4.0,2.0,2.5,7.46,0.0]|0.45|0.5148618166000156|\n",
      "|[293.0,97.0,2.0,2.0,4.0,7.8,1.0]  |0.64|0.5845742871098834|\n",
      "|[294.0,95.0,1.0,1.5,1.5,7.64,0.0] |0.49|0.4846116764044359|\n",
      "|[295.0,93.0,1.0,2.0,2.0,7.2,0.0]  |0.46|0.4295814877438666|\n",
      "|[295.0,99.0,2.0,2.5,3.0,7.65,0.0] |0.57|0.5288555619622342|\n",
      "|[295.0,101.0,2.0,2.5,2.0,7.86,0.0]|0.69|0.5424715944315557|\n",
      "|[296.0,97.0,2.0,1.5,2.0,7.8,0.0]  |0.49|0.5282532826703872|\n",
      "|[296.0,99.0,2.0,2.5,2.5,8.03,0.0] |0.61|0.5687751866038249|\n",
      "|[296.0,101.0,1.0,2.5,3.0,7.68,0.0]|0.6 |0.5377145102388599|\n",
      "+----------------------------------+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------------------------+----+-------------------+\n",
      "|features                          |cA  |prediction         |\n",
      "+----------------------------------+----+-------------------+\n",
      "|[294.0,93.0,1.0,1.5,2.0,7.36,0.0] |0.46|0.45185440159084966|\n",
      "|[295.0,96.0,2.0,1.5,2.0,7.34,0.0] |0.47|0.46441277021823146|\n",
      "|[296.0,95.0,2.0,3.0,2.0,7.54,1.0] |0.44|0.5014180043943184 |\n",
      "|[296.0,99.0,2.0,3.0,3.5,7.28,0.0] |0.47|0.4897996985108175 |\n",
      "|[298.0,92.0,1.0,2.0,2.0,7.88,0.0] |0.51|0.5173059907846305 |\n",
      "|[298.0,98.0,2.0,4.0,3.0,8.03,0.0] |0.34|0.5688787713670138 |\n",
      "|[298.0,101.0,2.0,1.5,2.0,7.86,0.0]|0.54|0.5525889952399594 |\n",
      "|[299.0,96.0,2.0,1.5,2.0,7.86,0.0] |0.54|0.5364313606818898 |\n",
      "|[300.0,99.0,1.0,1.0,2.5,8.01,0.0] |0.58|0.577413586849211  |\n",
      "|[301.0,98.0,1.0,2.0,3.0,8.03,1.0] |0.67|0.6045552546456838 |\n",
      "+----------------------------------+----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------------------------+----+-----------------+\n",
      "|features                          |cA  |prediction       |\n",
      "+----------------------------------+----+-----------------+\n",
      "|[337.0,118.0,4.0,4.5,4.5,9.65,1.0]|0.92|0.955110737114637|\n",
      "+----------------------------------+----+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_res.show(10,False)\n",
    "test.show(10,False)\n",
    "spcase.show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "#from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|prediction        |\n",
      "+------------------+\n",
      "|0.9278947368421054|\n",
      "+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtr = DecisionTreeRegressor(labelCol = 'cA' , featuresCol ='features')\n",
    "dtr_model = dtr.fit(model_df)\n",
    "dtr_predictions = dtr_model.transform(sp1_mod)\n",
    "\n",
    "dtr_predictions.select(['prediction']).show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized  Linear Regression\n",
    "\n",
    "follows exponential family type\n",
    "\n",
    "Family ----- Response Type ------ Suported\n",
    "Gaussian     Continious           Identity , Log , Inverse\n",
    "Binomial     Binary               logit  , probit , ClogLog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|prediction        |\n",
      "+------------------+\n",
      "|0.9514585639019955|\n",
      "+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "glr = GeneralizedLinearRegression(labelCol = 'cA' , featuresCol ='features' , family =\"gaussian\")\n",
    "glr_model = glr.fit(model_df)\n",
    "glr_predictions = glr_model.transform(sp1_mod)\n",
    "\n",
    "glr_predictions.select(['prediction']).show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----+-------------------+\n",
      "|features                          |cA  |prediction         |\n",
      "+----------------------------------+----+-------------------+\n",
      "|[290.0,100.0,1.0,1.5,2.0,7.56,0.0]|0.47|0.4810219874562627 |\n",
      "|[290.0,104.0,4.0,2.0,2.5,7.46,0.0]|0.45|0.5074803029108064 |\n",
      "|[293.0,97.0,2.0,2.0,4.0,7.8,1.0]  |0.64|0.579316395146293  |\n",
      "|[294.0,95.0,1.0,1.5,1.5,7.64,0.0] |0.49|0.4717123422791931 |\n",
      "|[295.0,93.0,1.0,2.0,2.0,7.2,0.0]  |0.46|0.42480121977535257|\n",
      "|[295.0,99.0,2.0,2.5,3.0,7.65,0.0] |0.57|0.5222586357363646 |\n",
      "|[295.0,101.0,2.0,2.5,2.0,7.86,0.0]|0.69|0.5307219472972107 |\n",
      "|[296.0,97.0,2.0,1.5,2.0,7.8,0.0]  |0.49|0.5169498534297157 |\n",
      "|[296.0,99.0,2.0,2.5,2.5,8.03,0.0] |0.61|0.5580164760100152 |\n",
      "|[296.0,101.0,1.0,2.5,3.0,7.68,0.0]|0.6 |0.5276867264142    |\n",
      "+----------------------------------+----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------------------------+----+-------------------+\n",
      "|features                          |cA  |prediction         |\n",
      "+----------------------------------+----+-------------------+\n",
      "|[294.0,93.0,1.0,1.5,2.0,7.36,0.0] |0.46|0.4437467053023756 |\n",
      "|[295.0,96.0,2.0,1.5,2.0,7.34,0.0] |0.47|0.45758071638719056|\n",
      "|[296.0,95.0,2.0,3.0,2.0,7.54,1.0] |0.44|0.4997537945884549 |\n",
      "|[296.0,99.0,2.0,3.0,3.5,7.28,0.0] |0.47|0.4895124286181751 |\n",
      "|[298.0,92.0,1.0,2.0,2.0,7.88,0.0] |0.51|0.5079727061173487 |\n",
      "|[298.0,98.0,2.0,4.0,3.0,8.03,0.0] |0.34|0.5647905324317153 |\n",
      "|[298.0,101.0,2.0,1.5,2.0,7.86,0.0]|0.54|0.5392393510552793 |\n",
      "|[299.0,96.0,2.0,1.5,2.0,7.86,0.0] |0.54|0.5263788785416206 |\n",
      "|[300.0,99.0,1.0,1.0,2.5,8.01,0.0] |0.58|0.5618284286547299 |\n",
      "|[301.0,98.0,1.0,2.0,3.0,8.03,1.0] |0.67|0.5954215533952583 |\n",
      "+----------------------------------+----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------------------------+----+------------------+\n",
      "|features                          |cA  |prediction        |\n",
      "+----------------------------------+----+------------------+\n",
      "|[337.0,118.0,4.0,4.5,4.5,9.65,1.0]|0.92|0.9514585639019955|\n",
      "+----------------------------------+----+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_res = glr_model.evaluate(train_df).predictions\n",
    "test = glr_model.evaluate(test_df).predictions\n",
    "spcase = glr_model.evaluate(sp1_mod).predictions\n",
    "train_res.show(10,False)\n",
    "test.show(10,False)\n",
    "spcase.show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Gradient Boosted Tree Regression\n",
    "\n",
    "Gradient-Boosted Trees (GBTs) are ensembles of decision trees. GBTs iteratively train decision trees in order to minimize a loss function.\n",
    "The spark.ml implementation supports GBTs for binary classification and for regression, using both continuous and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|prediction        |\n",
      "+------------------+\n",
      "|0.9267027715295116|\n",
      "+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbtr = GBTRegressor(labelCol = 'cA' , featuresCol ='features')\n",
    "gbtr_model = gbtr.fit(model_df)\n",
    "gbtr_predictions = gbtr_model.transform(sp1_mod)\n",
    "\n",
    "gbtr_predictions.select(['prediction']).show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isotonic regression\n",
    "\n",
    "We implement a pool adjacent violators algorithm which uses an approach to parallelizing isotonic regression. The training input is a DataFrame which contains three columns label, features and weight. Additionally IsotonicRegression algorithm has one optional parameter called isotonic defaulting to true. This argument specifies if the isotonic regression is isotonic (monotonically increasing) or antitonic (monotonically decreasing).\n",
    "\n",
    "Training returns an IsotonicRegressionModel that can be used to predict labels for both known and unknown features. The result of isotonic regression is treated as piecewise linear function. The rules for prediction therefore are:\n",
    "\n",
    "If the prediction input exactly matches a training feature then associated prediction is returned. In case there are multiple predictions with the same feature then one of them is returned. Which one is undefined (same as java.util.Arrays.binarySearch).\n",
    "If the prediction input is lower or higher than all training features then prediction with lowest or highest feature is returned respectively. In case there are multiple predictions with the same feature then the lowest or highest is returned respectively.\n",
    "If the prediction input falls between two training features then prediction is treated as piecewise linear function and interpolated value is calculated from the predictions of the two closest features. In case there are multiple values with the same feature then the same rules as in previous point are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import IsotonicRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|prediction        |\n",
      "+------------------+\n",
      "|0.9361111111111112|\n",
      "+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "afsr = IsotonicRegression(labelCol = 'cA' , featuresCol ='features')\n",
    "afsr_model = afsr.fit(model_df)\n",
    "afsr_predictions = afsr_model.transform(sp1_mod)\n",
    "\n",
    "afsr_predictions.select(['prediction']).show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boundaries in increasing order: [290.0,290.0,293.0,298.0,298.0,298.0,299.0,299.0,299.0,299.0,300.0,300.0,300.0,304.0,304.0,307.0,307.0,308.0,308.0,311.0,311.0,312.0,312.0,312.0,316.0,316.0,316.0,317.0,317.0,319.0,319.0,319.0,319.0,320.0,320.0,320.0,320.0,325.0,325.0,325.0,327.0,327.0,327.0,329.0,329.0,329.0,329.0,329.0,330.0,330.0,330.0,331.0,331.0,334.0,334.0,334.0,339.0,339.0,340.0,340.0,340.0,340.0]\n",
      "\n",
      "Predictions associated with the boundaries: [0.45,0.47,0.5005263157894738,0.5005263157894738,0.51,0.5119999999999999,0.5119999999999999,0.54,0.56,0.5766666666666667,0.5766666666666667,0.58,0.5800000000000001,0.5800000000000001,0.6245454545454545,0.6245454545454545,0.6571428571428573,0.6571428571428573,0.6573076923076923,0.6573076923076923,0.66,0.66,0.67,0.6790196078431372,0.6790196078431372,0.69,0.6950000000000001,0.6950000000000001,0.7172222222222222,0.7172222222222222,0.72,0.73,0.7371428571428572,0.7371428571428572,0.74,0.76,0.7880882352941176,0.7880882352941176,0.79,0.7954545454545454,0.7954545454545454,0.8,0.8275000000000001,0.8275000000000001,0.84,0.86,0.87,0.885,0.885,0.9,0.9020000000000001,0.9020000000000001,0.9149999999999999,0.9149999999999999,0.93,0.9361111111111112,0.9361111111111112,0.9399999999999998,0.9399999999999998,0.94,0.96,0.97]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Boundaries in increasing order: %s\\n\" % str(afsr_model.boundaries))\n",
    "print(\"Predictions associated with the boundaries: %s\\n\" % str(afsr_model.predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
