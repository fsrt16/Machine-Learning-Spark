{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|age|sex|cp |trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope|ca |thal|target|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|63 |1  |3  |145     |233 |1  |0      |150    |0    |2.3    |0    |0  |1   |1     |\n",
      "|37 |1  |2  |130     |250 |0  |1      |187    |0    |3.5    |0    |0  |2   |1     |\n",
      "|41 |0  |1  |130     |204 |0  |0      |172    |0    |1.4    |2    |0  |2   |1     |\n",
      "|56 |1  |1  |120     |236 |0  |1      |178    |0    |0.8    |2    |0  |2   |1     |\n",
      "|57 |0  |0  |120     |354 |0  |1      |163    |1    |0.6    |2    |0  |2   |1     |\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'sex',\n",
       " 'cp',\n",
       " 'trestbps',\n",
       " 'chol',\n",
       " 'fbs',\n",
       " 'restecg',\n",
       " 'thalach',\n",
       " 'exang',\n",
       " 'oldpeak',\n",
       " 'slope',\n",
       " 'ca',\n",
       " 'thal',\n",
       " 'target']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = pd.read_csv('Admission_Predict.csv')\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "#dataset_ps = spark.createDataFrame(dataset)\n",
    "df=spark.read.csv('heart.csv',inferSchema=True,header=True)\n",
    "d1 = pd.DataFrame([[66,1,1,160,246,0,1,120,1,0,1,3,1]] , columns =['age',\n",
    " 'sex',\n",
    " 'cp',\n",
    " 'trestbps',\n",
    " 'chol',\n",
    " 'fbs',\n",
    " 'restecg',\n",
    " 'thalach',\n",
    " 'exang',\n",
    " 'oldpeak',\n",
    " 'slope',\n",
    " 'ca',\n",
    " 'thal'] )\n",
    "sp = spark.createDataFrame(d1)\n",
    "d1\n",
    "df.show(5,False)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VECTORIZE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import VectorAssembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature------>\n",
      "DataFrame[age: int, sex: int, cp: int, trestbps: int, chol: int, fbs: int, restecg: int, thalach: int, exang: int, oldpeak: double, slope: int, ca: int, thal: int, target: int, features: vector]\n",
      "EXPERIMENTAL SP------->\n",
      "DataFrame[age: bigint, sex: bigint, cp: bigint, trestbps: bigint, chol: bigint, fbs: bigint, restecg: bigint, thalach: bigint, exang: bigint, oldpeak: bigint, slope: bigint, ca: bigint, thal: bigint, features: vector]\n"
     ]
    }
   ],
   "source": [
    "vc = VectorAssembler(inputCols = ['age',\n",
    " 'sex',\n",
    " 'cp',\n",
    " 'trestbps',\n",
    " 'chol',\n",
    " 'fbs',\n",
    " 'restecg',\n",
    " 'thalach',\n",
    " 'exang',\n",
    " 'oldpeak',\n",
    " 'slope',\n",
    " 'ca',\n",
    " 'thal'] \n",
    "                     , outputCol = 'features')\n",
    "features_df = vc.transform(df)\n",
    "print(\"Feature------>\")\n",
    "print(features_df)\n",
    "sp1=vc.transform(sp)\n",
    "print(\"EXPERIMENTAL SP------->\")\n",
    "print(sp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[features: vector, target: int]\n",
      "DataFrame[features: vector, target: int]\n",
      "test:--\n",
      "DataFrame[features: vector, target: int]\n",
      "train:--\n",
      "DataFrame[features: vector, target: int]\n",
      "model:--\n",
      "DataFrame[features: vector, target: int]\n"
     ]
    }
   ],
   "source": [
    "model_df  = features_df.select('features','target') \n",
    "sp1_mod = features_df.select('features','target')\n",
    "print(sp1_mod)\n",
    "print(model_df)\n",
    "train_df , test_df = model_df.randomSplit([0.7,0.3])\n",
    "print(\"test:--\")\n",
    "print(test_df)\n",
    "print(\"train:--\")\n",
    "print(train_df)\n",
    "print(\"model:--\")\n",
    "print(model_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION MODEL --- >  Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = LogisticRegression(maxIter=10,regParam=0.3,elasticNetParam=0.8,featuresCol = 'features',labelCol='target')\n",
    "nb_m = nb.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|1.0       |\n",
      "+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_predictions = nb_m.transform(sp1_mod)\n",
    "dt_predictions.select(['prediction']).show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------------------------------------+\n",
      "|prediction|target|probability                            |\n",
      "+----------+------+---------------------------------------+\n",
      "|1.0       |1     |[0.4285714285714286,0.5714285714285714]|\n",
      "|1.0       |1     |[0.4285714285714286,0.5714285714285714]|\n",
      "|1.0       |1     |[0.4285714285714286,0.5714285714285714]|\n",
      "|1.0       |0     |[0.4285714285714286,0.5714285714285714]|\n",
      "|1.0       |1     |[0.4285714285714286,0.5714285714285714]|\n",
      "|1.0       |1     |[0.4285714285714286,0.5714285714285714]|\n",
      "|1.0       |1     |[0.4285714285714286,0.5714285714285714]|\n",
      "|1.0       |0     |[0.4285714285714286,0.5714285714285714]|\n",
      "|1.0       |1     |[0.4285714285714286,0.5714285714285714]|\n",
      "|1.0       |1     |[0.4285714285714286,0.5714285714285714]|\n",
      "|1.0       |1     |[0.4285714285714286,0.5714285714285714]|\n",
      "|1.0       |0     |[0.4285714285714286,0.5714285714285714]|\n",
      "|1.0       |1     |[0.4285714285714286,0.5714285714285714]|\n",
      "|1.0       |1     |[0.4285714285714286,0.5714285714285714]|\n",
      "|1.0       |1     |[0.4285714285714286,0.5714285714285714]|\n",
      "|1.0       |1     |[0.4285714285714286,0.5714285714285714]|\n",
      "|1.0       |1     |[0.4285714285714286,0.5714285714285714]|\n",
      "|1.0       |1     |[0.4285714285714286,0.5714285714285714]|\n",
      "|1.0       |1     |[0.4285714285714286,0.5714285714285714]|\n",
      "|1.0       |1     |[0.4285714285714286,0.5714285714285714]|\n",
      "+----------+------+---------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_predictions = nb_m.transform(test_df)\n",
    "dt_predictions.select(['prediction','target','probability']).show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CONFUSION MATRIX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "true_postives = dt_predictions[(dt_predictions.target == 1) & (dt_predictions.prediction == 1)].count()\n",
    "true_negatives = dt_predictions[(dt_predictions.target == 0) & (dt_predictions.prediction == 0)].count()\n",
    "false_positives = dt_predictions[(dt_predictions.target == 0) & (dt_predictions.prediction == 1)].count()\n",
    "false_negatives = dt_predictions[(dt_predictions.target == 1) & (dt_predictions.prediction == 0)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "0\n",
      "41\n",
      "0\n",
      "97\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "print (true_postives)\n",
    "print (true_negatives)\n",
    "print (false_positives)\n",
    "print (false_negatives)\n",
    "print(true_postives+true_negatives+false_positives+false_negatives)\n",
    "print (dt_predictions.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall---->\n",
      "1.0\n",
      "precision---->\n",
      "0.5773195876288659\n",
      "accuracy------>\n",
      "0.5773195876288659\n"
     ]
    }
   ],
   "source": [
    "recall = float(true_postives)/(true_postives + false_negatives)\n",
    "print('recall---->')\n",
    "print(recall)\n",
    "precision = float(true_postives) / (true_postives + false_positives)\n",
    "print('precision---->')\n",
    "print(precision)\n",
    "accuracy=float((true_postives+true_negatives) /(dt_predictions.count()))\n",
    "print('accuracy------>')\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Decision Tree Classifier\n",
    "\n",
    "Decision trees are widely used since they are easy to interpret, handle categorical features, extend to the multi-class classification, do not require feature scaling, and are able to capture non-linearities and feature interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+\n",
      "|target|prediction|         probability|\n",
      "+------+----------+--------------------+\n",
      "|     1|       1.0|[0.28571428571428...|\n",
      "|     1|       1.0|[0.28571428571428...|\n",
      "|     1|       1.0|[0.28571428571428...|\n",
      "|     0|       0.0|[0.96078431372549...|\n",
      "|     1|       1.0|[0.12345679012345...|\n",
      "|     1|       1.0|[0.12345679012345...|\n",
      "|     1|       1.0|[0.28571428571428...|\n",
      "|     0|       0.0|[0.88888888888888...|\n",
      "|     1|       1.0|[0.12345679012345...|\n",
      "|     1|       1.0|[0.12345679012345...|\n",
      "+------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'target', maxDepth = 3)\n",
    "dtModel = dt.fit(train_df)\n",
    "predictions = dtModel.transform(test_df)\n",
    "predictions.select('target', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+\n",
      "|target|prediction|         probability|\n",
      "+------+----------+--------------------+\n",
      "|     1|       1.0|[0.18407024145744...|\n",
      "|     1|       1.0|[0.18379644790603...|\n",
      "|     1|       1.0|[0.30735436976875...|\n",
      "|     0|       0.0|[0.68320485316552...|\n",
      "|     1|       1.0|[0.03701892560111...|\n",
      "|     1|       1.0|[0.08493559226777...|\n",
      "|     1|       1.0|[0.24076775963013...|\n",
      "|     0|       0.0|[0.67157151518707...|\n",
      "|     1|       1.0|[0.04471123329341...|\n",
      "|     1|       1.0|[0.19592436259937...|\n",
      "+------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'target')\n",
    "rfModel = rf.fit(train_df)\n",
    "predictions = rfModel.transform(test_df)\n",
    "predictions.select('target', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-Boosted Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+\n",
      "|target|prediction|         probability|\n",
      "+------+----------+--------------------+\n",
      "|     1|       1.0|[0.06012673056092...|\n",
      "|     1|       1.0|[0.08384677258236...|\n",
      "|     1|       1.0|[0.10590518741031...|\n",
      "|     0|       0.0|[0.92032306215624...|\n",
      "|     1|       1.0|[0.06800352292569...|\n",
      "|     1|       1.0|[0.06800352292569...|\n",
      "|     1|       1.0|[0.02871550721821...|\n",
      "|     0|       0.0|[0.86236790596391...|\n",
      "|     1|       1.0|[0.06695369239070...|\n",
      "|     1|       1.0|[0.09464001356760...|\n",
      "+------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(maxIter=10,labelCol = 'target')\n",
    "gbtModel = gbt.fit(train_df)\n",
    "predictions = gbtModel.transform(test_df)\n",
    "predictions.select('target', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Linear Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+----------+\n",
      "|            features|target|       rawPrediction|prediction|\n",
      "+--------------------+------+--------------------+----------+\n",
      "|(13,[0,1,3,4,7,10...|     0|[-0.4313503278604...|       1.0|\n",
      "|(13,[0,1,3,4,7,10...|     1|[-1.5252154187336...|       1.0|\n",
      "|(13,[0,1,3,4,7,10...|     1|[-1.3886456399779...|       1.0|\n",
      "|(13,[0,3,4,7,9,10...|     1|[-0.5704845883426...|       1.0|\n",
      "|(13,[0,3,4,7,9,10...|     1|[-0.7671796752333...|       1.0|\n",
      "|(13,[0,3,4,7,9,11...|     0|[1.57293125998882...|       0.0|\n",
      "|(13,[0,3,4,7,9,11...|     0|[4.28869630823163...|       0.0|\n",
      "|[29.0,1.0,1.0,130...|     1|[-2.3184720405181...|       1.0|\n",
      "|[34.0,1.0,3.0,118...|     1|[-2.9282014190577...|       1.0|\n",
      "|[35.0,0.0,0.0,138...|     1|[-1.6062222734503...|       1.0|\n",
      "+--------------------+------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "lsvc = LinearSVC(featuresCol = 'features', labelCol = 'target')\n",
    "lsvcModel = lsvc.fit(train_df)\n",
    "predictions = lsvcModel.transform(test_df)\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|target|prediction|\n",
      "+------+----------+\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(featuresCol = 'features', labelCol = 'target')\n",
    "nbModel = nb.fit(train_df)\n",
    "predictions = nbModel.transform(test_df)\n",
    "predictions.select('target', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
